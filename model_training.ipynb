{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to train the model.\n",
    "Uses attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "from lib.models.EEG_Net_CNN import EEG_Net_CNN\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.utils import load_data, train, test\n",
    "from lib.Datasets import EEGDataset\n",
    "from lib.DataObject import DataObject\n",
    "import lib.DataObjectUtils as util\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from lib.DataHandler import DataAcquisitionHandler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 250])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "filepath = \"C:/Users/c25th/code/P300_BCI_Speller/\"\n",
    "date = \"2023-12-04\"\n",
    "\n",
    "train_filename = \"data/dataloaders/train_loader_\"+str(date)+\".txt\"\n",
    "val_filename = \"data/dataloaders/val_loader_\"+str(date)+\".txt\"\n",
    "test_filename = \"data/dataloaders/test_loader_\"+str(date)+\".txt\"\n",
    "\n",
    "train_loader = torch.load(filepath + train_filename)\n",
    "val_loader = torch.load(filepath + val_filename)\n",
    "test_loader = torch.load(filepath + test_filename)\n",
    "\n",
    "# Print dimentions of data\n",
    "for s, l in train_loader:\n",
    "    print(s.size())\n",
    "    print(l.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create p300Model\n",
    "class EEG_Net_Attention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Expecting input of shape (batch_size, channels, readings)\n",
    "    input = [32, 8, 250] = [batch_size, channels, readings]\n",
    "    batch_size: number of samples in a batch\n",
    "    channels: number of channels in a sample (8)\n",
    "    readings: number of readings in a channel (len())\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels=8, num_classes=2, input_length=250):\n",
    "        super(EEG_Net_CNN, self).__init__()\n",
    "\n",
    "    # Attention\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = EEG_Net_Attention()\n",
    "\n",
    "# loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 7.4829 - Train Accuracy: 0.4903 - Val Loss: 2.6890 - Val Accuracy: 0.4638\n",
      "Epoch 2 - Train Loss: 1.4534 - Train Accuracy: 0.5129 - Val Loss: 1.0350 - Val Accuracy: 0.5326\n",
      "Epoch 3 - Train Loss: 0.8066 - Train Accuracy: 0.5007 - Val Loss: 0.7142 - Val Accuracy: 0.5362\n",
      "Epoch 4 - Train Loss: 0.7319 - Train Accuracy: 0.4817 - Val Loss: 0.7011 - Val Accuracy: 0.5254\n",
      "Epoch 5 - Train Loss: 0.7248 - Train Accuracy: 0.4952 - Val Loss: 0.7158 - Val Accuracy: 0.4493\n",
      "Epoch 6 - Train Loss: 0.7057 - Train Accuracy: 0.5088 - Val Loss: 0.6968 - Val Accuracy: 0.4746\n",
      "Epoch 7 - Train Loss: 0.6943 - Train Accuracy: 0.5061 - Val Loss: 0.6955 - Val Accuracy: 0.4493\n",
      "Epoch 8 - Train Loss: 0.6932 - Train Accuracy: 0.5129 - Val Loss: 0.6941 - Val Accuracy: 0.4493\n",
      "Epoch 9 - Train Loss: 0.6933 - Train Accuracy: 0.5129 - Val Loss: 0.6947 - Val Accuracy: 0.4493\n",
      "Epoch 10 - Train Loss: 0.6928 - Train Accuracy: 0.5111 - Val Loss: 0.6961 - Val Accuracy: 0.4493\n",
      "Epoch 11 - Train Loss: 0.6928 - Train Accuracy: 0.5129 - Val Loss: 0.6985 - Val Accuracy: 0.4493\n",
      "Epoch 12 - Train Loss: 0.6934 - Train Accuracy: 0.5129 - Val Loss: 0.6948 - Val Accuracy: 0.4493\n",
      "Epoch 13 - Train Loss: 0.6928 - Train Accuracy: 0.5129 - Val Loss: 0.6948 - Val Accuracy: 0.4493\n",
      "Epoch 14 - Train Loss: 0.6933 - Train Accuracy: 0.5129 - Val Loss: 0.6954 - Val Accuracy: 0.4493\n",
      "Epoch 15 - Train Loss: 0.6926 - Train Accuracy: 0.5129 - Val Loss: 0.6959 - Val Accuracy: 0.4493\n",
      "Epoch 16 - Train Loss: 0.6931 - Train Accuracy: 0.5129 - Val Loss: 0.6958 - Val Accuracy: 0.4493\n",
      "Epoch 17 - Train Loss: 0.6927 - Train Accuracy: 0.5129 - Val Loss: 0.6956 - Val Accuracy: 0.4493\n",
      "Epoch 18 - Train Loss: 0.6931 - Train Accuracy: 0.5129 - Val Loss: 0.6962 - Val Accuracy: 0.4493\n",
      "Epoch 19 - Train Loss: 0.6933 - Train Accuracy: 0.5129 - Val Loss: 0.6952 - Val Accuracy: 0.4493\n",
      "Epoch 20 - Train Loss: 0.6927 - Train Accuracy: 0.5129 - Val Loss: 0.6954 - Val Accuracy: 0.4493\n",
      "Epoch 21 - Train Loss: 0.6926 - Train Accuracy: 0.5129 - Val Loss: 0.6960 - Val Accuracy: 0.4493\n",
      "Epoch 22 - Train Loss: 0.6928 - Train Accuracy: 0.5147 - Val Loss: 0.6969 - Val Accuracy: 0.4493\n",
      "Epoch 23 - Train Loss: 0.6933 - Train Accuracy: 0.5129 - Val Loss: 0.6966 - Val Accuracy: 0.4493\n",
      "Epoch 24 - Train Loss: 0.6925 - Train Accuracy: 0.5129 - Val Loss: 0.6966 - Val Accuracy: 0.4493\n",
      "Epoch 25 - Train Loss: 0.6924 - Train Accuracy: 0.5129 - Val Loss: 0.6967 - Val Accuracy: 0.4493\n",
      "Epoch 26 - Train Loss: 0.6938 - Train Accuracy: 0.4989 - Val Loss: 0.6971 - Val Accuracy: 0.4493\n",
      "Epoch 27 - Train Loss: 0.6934 - Train Accuracy: 0.5134 - Val Loss: 0.6973 - Val Accuracy: 0.4493\n",
      "Epoch 28 - Train Loss: 0.6934 - Train Accuracy: 0.5097 - Val Loss: 0.6968 - Val Accuracy: 0.4493\n",
      "Epoch 29 - Train Loss: 0.6936 - Train Accuracy: 0.5129 - Val Loss: 0.6966 - Val Accuracy: 0.4493\n",
      "Epoch 30 - Train Loss: 0.6927 - Train Accuracy: 0.5129 - Val Loss: 0.6963 - Val Accuracy: 0.4493\n",
      "Epoch 31 - Train Loss: 0.6933 - Train Accuracy: 0.5097 - Val Loss: 0.6960 - Val Accuracy: 0.4819\n",
      "Epoch 32 - Train Loss: 0.6928 - Train Accuracy: 0.5170 - Val Loss: 0.6969 - Val Accuracy: 0.4493\n",
      "Epoch 33 - Train Loss: 0.6924 - Train Accuracy: 0.5129 - Val Loss: 0.6977 - Val Accuracy: 0.4493\n",
      "Epoch 34 - Train Loss: 0.6935 - Train Accuracy: 0.5129 - Val Loss: 0.6970 - Val Accuracy: 0.4493\n",
      "Epoch 35 - Train Loss: 0.6933 - Train Accuracy: 0.5129 - Val Loss: 0.6971 - Val Accuracy: 0.4493\n",
      "Epoch 36 - Train Loss: 0.6925 - Train Accuracy: 0.5129 - Val Loss: 0.6962 - Val Accuracy: 0.4493\n",
      "Epoch 37 - Train Loss: 0.6933 - Train Accuracy: 0.5129 - Val Loss: 0.6961 - Val Accuracy: 0.4493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\c25th\\code\\P300_BCI_Speller\\model_training.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/c25th/code/P300_BCI_Speller/model_training.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# def train_and_validate(train_dataloader, val_dataloader, model, loss_fn, optimizer, num_epochs, print_every=100):\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/c25th/code/P300_BCI_Speller/model_training.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_losses, val_losses, train_accuracies, val_accuracies \u001b[39m=\u001b[39m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/c25th/code/P300_BCI_Speller/model_training.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_loader, val_loader, model, loss_fn, optimizer, \u001b[39m100\u001b[39;49m, print_every\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\c25th\\code\\P300_BCI_Speller\\lib\\utils.py:25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, val_dataloader, model, loss_fn, optimizer, num_epochs, print_every)\u001b[0m\n\u001b[0;32m     23\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     24\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 25\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     27\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     28\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\c25th\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\c25th\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\c25th\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\c25th\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\c25th\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m--> 434\u001b[0m     param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m amsgrad \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train & test model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train(\n",
    "    train_loader, val_loader, model, loss_fn, optimizer, 100)\n",
    "print(\"Maximum Train Accuracy: \", max(train_accuracies))\n",
    "test(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Train Accuracy:  0.536441828881847\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 0.701727 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum Train Accuracy: \", max(train_accuracies))\n",
    "test(test_loader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
