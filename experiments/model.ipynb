{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File to experiment with different machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from lib.DataObject import DataObject\n",
    "import lib.DataObjectUtils as util\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from lib.DataHandler import DataAcquisitionHandler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from data/* using pickle\n",
    "filename = 'data/handler_box_data_full_Oct_30_2023.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    handler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of samples: 83\n",
      "Sample - (channels, label): 2\n",
      "Channels: 24\n",
      "Max Channel Len - [reading_1, ...]: 250\n",
      "Min Channel Len - [reading_1, ...]: 250\n",
      "Sample example: (tensor([[ 2.4500e+02,  2.4600e+02,  2.4700e+02,  ...,  2.3600e+02,\n",
      "          2.3700e+02,  2.3800e+02],\n",
      "        [ 8.3703e+03,  8.4933e+03,  8.5348e+03,  ...,  8.4436e+03,\n",
      "          8.5442e+03,  8.3925e+03],\n",
      "        [-6.4812e+00, -6.1533e+00, -5.2448e+00,  ..., -3.8378e+00,\n",
      "         -3.7535e+00, -3.5997e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.6987e+09,  1.6987e+09,  1.6987e+09,  ...,  1.6987e+09,\n",
      "          1.6987e+09,  1.6987e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "# Data filter visitor test\n",
    "\n",
    "data = DataObject(handler.get_data())\n",
    "\n",
    "data.accept(util.BandpassFilterVisitor(low=0.1, high=10))\n",
    "data.accept(util.BandstopFilterVisitor(low=49, high=51))\n",
    "\n",
    "key_data, box_data = data.get_data(decorator=util.MakeTensorWindowsDataDecorator())\n",
    "\n",
    "sample = box_data[0]\n",
    "\n",
    "print(\"List of samples:\" ,len(box_data))\n",
    "print(\"Sample - (channels, label):\", len(box_data[0]))\n",
    "print(\"Channels:\", len(box_data[0][0]))\n",
    "max_channel_len = 0\n",
    "min_channel_len = 10000000\n",
    "for sample in box_data:\n",
    "    for channel in sample[0]:\n",
    "        if len(channel) > max_channel_len:\n",
    "            max_channel_len = len(channel)\n",
    "        if len(channel) < min_channel_len:\n",
    "            min_channel_len = len(channel)\n",
    "print(\"Max Channel Len - [reading_1, ...]:\", max_channel_len)\n",
    "print(\"Min Channel Len - [reading_1, ...]:\", min_channel_len)\n",
    "print(\"Sample example:\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "class EEGDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data, self.labels = self.parse_data(data)\n",
    "        self.window_size = self.data.shape[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "    def parse_data(self, data):\n",
    "        \"\"\"\n",
    "        Data comes in the form of a list of tuples (data, label)\n",
    "        data is a 2D tensor (channels, readings)\n",
    "        label is a 1D tensor (1)\n",
    "\n",
    "        return:\n",
    "        data: 3D tensor (samples, channels, readings)\n",
    "        label: 1D tensor (samples)\n",
    "        \"\"\"\n",
    "\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "\n",
    "        channels_idx = (1, 9)\n",
    "\n",
    "        for sample in data:\n",
    "            data_list.append(sample[0][channels_idx[0]:channels_idx[1]])\n",
    "            label_list.append(sample[1])\n",
    "\n",
    "        return torch.stack(data_list), torch.stack(label_list)\n",
    "    \n",
    "    def downsample(self):\n",
    "        # Count the number of occurrences of each class\n",
    "        label_counts = torch.bincount(self.labels)\n",
    "        # Find the least represented class and its count\n",
    "        min_count = torch.min(label_counts).item()\n",
    "\n",
    "        downsampled_data = []\n",
    "        downsampled_labels = []\n",
    "        # Keep track of how many samples per class are added to the downsampled set\n",
    "        samples_per_class = dict()\n",
    "\n",
    "        # Iterate over data and add samples to the new downsampled dataset\n",
    "        for i in range(len(self.data)):\n",
    "            label = self.labels[i].item()\n",
    "            # If the class is not in the dictionary, or the count for this class is less than the minimum count\n",
    "            if samples_per_class.get(label, 0) < min_count:\n",
    "                # Add the sample to the downsampled set\n",
    "                downsampled_data.append(self.data[i])\n",
    "                downsampled_labels.append(self.labels[i])\n",
    "                # Increment the count for this class in the dictionary\n",
    "                samples_per_class[label] = samples_per_class.get(label, 0) + 1\n",
    "\n",
    "        # Replace the dataset with the downsampled set, stacked into tensors\n",
    "        self.data = torch.stack(downsampled_data)\n",
    "        self.labels = torch.stack(downsampled_labels)\n",
    "\n",
    "    # Function to create a balanced sampler\n",
    "    def make_balanced_sampler(labels):\n",
    "        class_counts = torch.bincount(labels)\n",
    "        class_weights = 1. / class_counts\n",
    "        weights = class_weights[labels]\n",
    "        sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "    # Function to print class distribution per batch\n",
    "    def print_class_distribution_per_batch(dataloader):\n",
    "        for i, (_, labels) in enumerate(dataloader):\n",
    "            class_counts = torch.bincount(labels)\n",
    "            class_distribution = {f\"class_{class_idx}\": count.item() for class_idx, count in enumerate(class_counts)}\n",
    "            print(f\"Batch {i}: class distribution: {class_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 62\n",
      "Test dataset size: 16\n",
      "Train loader size: 2\n",
      "Test loader size: 1\n",
      "Sample data shape: 78\n",
      "Batch 0: class distribution: {'class_0': 16, 'class_1': 16}\n",
      "Batch 1: class distribution: {'class_0': 15, 'class_1': 15}\n"
     ]
    }
   ],
   "source": [
    "# Data loader\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset = EEGDataset(box_data)\n",
    "dataset.downsample()\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check if data is loaded correctly\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "print(\"Train loader size:\", len(train_loader))\n",
    "print(\"Test loader size:\", len(test_loader))\n",
    "print(\"Sample data shape:\", len(dataset))\n",
    "\n",
    "def print_class_distribution_per_batch(dataloader):\n",
    "    for i, (_, labels) in enumerate(dataloader):\n",
    "        # Counting occurrences of each class in the batch\n",
    "        class_counts = torch.bincount(labels)\n",
    "        class_distribution = {f\"class_{class_idx}\": count.item() for class_idx, count in enumerate(class_counts)}\n",
    "        print(f\"Batch {i}: class distribution: {class_distribution}\")\n",
    "\n",
    "# Call the function for your train_loader\n",
    "print_class_distribution_per_batch(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create p300Model\n",
    "class EEG_Net_CNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation of EEGNet\n",
    "\n",
    "    Expecting input of shape (batch_size, channels, readings)\n",
    "    input = [32, 8, 250] = [batch_size, channels, readings]\n",
    "    batch_size: number of samples in a batch\n",
    "    channels: number of channels in a sample (8)\n",
    "    readings: number of readings in a channel (len())\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels=8, num_classes=2, input_length=250):\n",
    "        super(EEG_Net_CNN, self).__init__()\n",
    "\n",
    "        self.block1 = torch.nn.Sequential(\n",
    "            # Conv1D\n",
    "            nn.Conv1d(num_channels, 32, kernel_size=50, stride=1, padding=0, bias=False),\n",
    "            \n",
    "            # Batch norm\n",
    "            nn.BatchNorm1d(32),\n",
    "\n",
    "            # DepthwiseConv1D\n",
    "            nn.Conv1d(32, 32, kernel_size=1, groups=32, bias=False),\n",
    "\n",
    "            # Batch norm\n",
    "            nn.BatchNorm1d(32),\n",
    "\n",
    "            # ELU Activation\n",
    "            nn.ELU(alpha=1.0),\n",
    "\n",
    "            # Avg Pooling 1D\n",
    "            nn.AvgPool1d(kernel_size=4, stride=4, padding=0),\n",
    "\n",
    "            # Dropout\n",
    "            nn.Dropout(p=0.15)\n",
    "        )\n",
    "\n",
    "        self.block2 = torch.nn.Sequential( \n",
    "            # Separable Conv1D\n",
    "            nn.Conv1d(32, 32, kernel_size=15, stride=1, padding=0, bias=False),\n",
    "\n",
    "            # Batch norm\n",
    "            nn.BatchNorm1d(32),\n",
    "\n",
    "            # ELU Activation\n",
    "            nn.ELU(alpha=1.0),\n",
    "\n",
    "            # Avg Pooling 1D\n",
    "            nn.AvgPool1d(kernel_size=8, stride=8, padding=0),\n",
    "\n",
    "            # Dropout\n",
    "            nn.Dropout(p=0.15)\n",
    "        )\n",
    "\n",
    "        # Calculating the length of the signal after convolutions and pooling\n",
    "        def conv_output_length(input_length, kernel_size, stride=1, padding=0):\n",
    "            return (input_length - kernel_size + 2*padding) // stride + 1\n",
    "        \n",
    "        conv1_out_length = conv_output_length(input_length, 50, 1, 0)\n",
    "        pool1_out_length = conv_output_length(conv1_out_length, 4, 4, 0)\n",
    "        conv2_out_length = conv_output_length(pool1_out_length, 15, 1, 0)\n",
    "        pool2_out_length = conv_output_length(conv2_out_length, 8, 8, 0)\n",
    "        \n",
    "        linear_input_features = pool2_out_length * 32  # 32 is the number of output channels after block1\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(in_features=linear_input_features, out_features=num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # block 1\n",
    "        x = self.block1(x)\n",
    "\n",
    "        # block 2\n",
    "        x = self.block2(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = EEG_Net_CNN()\n",
    "\n",
    "# loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, model, loss_fn, optimizer, num_epochs, print_every=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "            if batch % print_every == 0 and print_every > 0:\n",
    "                print(f'Epoch {epoch+1} - Batch {batch+1}/{len(train_dataloader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        avg_train_acc = correct / total\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_dataloader:\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                total += y.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        avg_val_acc = correct / total\n",
    "        \n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(avg_val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Train Accuracy: {avg_train_acc:.4f} - Val Loss: {avg_val_loss:.4f} - Val Accuracy: {avg_val_acc:.4f}')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.plot(train_accuracies, label='Train')\n",
    "    plt.plot(val_accuracies, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, train_accuracies, val_accuracies = train(\n",
    "    train_loader, test_loader, model, loss_fn, optimizer, 10, print_every=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
